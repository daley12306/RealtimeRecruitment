# ğŸ“Š Realtime Recruitment Analytics Platform

Realtime Recruitment Analytics Platform is a real-time data pipeline that simulates and visualizes job application processes using modern data engineering tools. It generates synthetic candidate data, processes application logic, aggregates metrics via Apache Spark Streaming, and delivers insights through an interactive Streamlit dashboard â€” all orchestrated with Docker containers. This project demonstrates how to build a scalable streaming system from data ingestion to real-time visualization.

---

## ğŸ› ï¸ Technologies

* Kafka: Real-time message broker
* PostgreSQL: Persistent data store
* Apache Spark (Structured Streaming): Real-time data aggregation
* Streamlit: Interactive dashboard
* Docker Compose: Service orchestration

---

## ğŸš€ Architecture Overview

![Architecture](images/architecture.png)

---

## ğŸ§± Main Components

| Component            | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| `candidate.py`       | Generates random candidate data from API, pushes to Kafka & PostgreSQL     |
| `application.py`     | Listens for candidates, selects random position, and pushes application     |
| `spark_streaming.py` | Consumes applications, performs aggregation, and sends to Kafka topics      |
| `streamlit_app.py`   | Displays live dashboard from Kafka and PostgreSQL data                      |

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ candidate.py
â”‚ â”œâ”€â”€ application.py
â”‚ â”œâ”€â”€ spark_streaming.py
â”‚ â””â”€â”€ streamlit_app.py
â”œâ”€â”€ init.sql
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.base
â”œâ”€â”€ Dockerfile.streaming
â”œâ”€â”€ images/ # (Optional) for dashboard screenshots
â””â”€â”€ README.md
```

---

## âœ… Features

* Real-time data pipeline with Kafka + Spark Structured Streaming
* Persistent storage in PostgreSQL
* Live and auto-refreshing dashboard built with Streamlit + Plotly
* Application insights:
    * Per position
    * By score range
    * By years of experience

---

## ğŸ“¸ Dashboard Example

![Dashboard 1](images/dashboard1.png)
![Dashboard 2](images/dashboard2.png)

---

## ğŸ³ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/daley12306/RealtimeRecruitment.git
cd RealtimeRecruitment
```

### 2. Build & Start Docker containers:

To build and start all services:

```bash
docker compose up --build -d
```
To scale Spark workers (e.g., to run with 3 workers):

```bash
docker compose up --scale spark-worker=3 --build -d
```

### 3. Access the Spark UI

Open your browser at: [http://localhost:8080](http://localhost:8080)

### 4. Access the dashboard

Open your browser at: [http://localhost:8501](http://localhost:8501)

---

## ğŸ›¢ï¸ PostgreSQL:

The `init.sql` script initializes the `candidate`, `position`, and `application` tables, along with some default positions.

### Credentials:

* User: `postgres`
* Password: `secret`

### Access PostgreSQL manually:

```bash
docker exec -it postgres psql -U postgres -d recruitment
```

---

## ğŸ”— Kafka CLI

You can inspect and consume Kafka topics using the following commands (from inside a container that has Kafka tools installed):

### List all topics:

```bash
docker exec -it broker kafka-topics --list --bootstrap-server broker:29092
```

### Consume messages from a specific topic (e.g., `applications_per_position`):

```bash
docker exec -it broker kafka-console-consumer --topic applications_per_position --bootstrap-server broker:29092
```

---

## ğŸª© Clean Up

To shut down and remove all containers and volumes:

```bash
docker compose down -v
```
